import requests
import logging


class LLMIntegration:
    def __init__(self, base_url="http://localhost:11434", model="llama3.1:latest", timeout=30):
        self.base_url = base_url
        self.model = model
        self.timeout = timeout
        logging.basicConfig(level=logging.INFO,
                            format="%(asctime)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger("LLMIntegration")

    def call_generate_endpoint(self, prompt):
        """
        Interact with the /api/generate endpoint for streaming-like tasks.
        """
        url = f"{self.base_url}/api/generate"
        payload = {"model": self.model, "prompt": prompt}

        try:
            response = requests.post(url, json=payload, timeout=self.timeout)
            response.raise_for_status()

            # Collect the streaming output
            result = ""
            for chunk in response.json():
                result += chunk.get("response", "")
                if chunk.get("done", False):
                    break
            return result.strip()

        except requests.RequestException as e:
            self.logger.error(
                f"Error communicating with the LLM (generate): {e}")
            return None

    def call_chat_completions_endpoint(self, messages):
        """
        Interact with the /v1/chat/completions endpoint for structured chat tasks.
        """
        url = f"{self.base_url}/v1/chat/completions"
        payload = {"model": self.model, "messages": messages}

        try:
            response = requests.post(url, json=payload, timeout=self.timeout)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()

        except requests.RequestException as e:
            self.logger.error(
                f"Error communicating with the LLM (chat completions): {e}")
            return None

    def reformulate_query(self, query):
        """
        Reformulate user queries using the chat completions endpoint.
        """
        messages = [
            {"role": "user", "content": f"Reformulate the query: {query}"}]
        return self.call_chat_completions_endpoint(messages)

    def analyze_content(self, content):
        """
        Analyze and summarize scraped content using the chat completions endpoint.
        """
        messages = [
            {"role": "user", "content": f"Analyze this content: {content}"}]
        return self.call_chat_completions_endpoint(messages)

def generate_final_answer(self, query, content):
    """
    Generate a final answer using the query and scraped content.

    Args:
        query (str): The user's search query.
        content (str): The content scraped from the web.

    Returns:
        str: The final answer generated by the LLM.
    """
    prompt = (
        f"Based on the following query and content, provide a comprehensive answer:\n\n"
        f"Query: {query}\n\nContent:\n{content}\n\nAnswer:"
    )

    payload = {
        "model": self.model,
        "prompt": prompt
    }

    # Log the payload for debugging
    self.logger.info(f"Payload sent to /api/generate: {payload}")

    try:
        response = requests.post(
            f"{self.base_url}/api/generate",
            json=payload,
            timeout=self.timeout
        )
        response.raise_for_status()
        data = response.json()

        # Extract the response text
        return data.get("response", "No response generated.")
    except requests.RequestException as e:
        self.logger.error(f"Error communicating with the LLM (generate): {e}")
        return None
    except ValueError as e:
        self.logger.error(f"Error parsing response JSON (generate): {e}")
        return None
